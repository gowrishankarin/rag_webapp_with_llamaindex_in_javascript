{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245cb414-6570-405d-87c0-0292b5ddf6b1",
   "metadata": {},
   "source": [
    "Documents - Text objects, within a corpus of documents\n",
    "Document -> Node structure -> Structure indices\n",
    "https://www.youtube.com/watch?v=wYZJq8CNmTw\n",
    "https://colab.research.google.com/drive/1JOzbVzrm8_GJAmuh2Qcjsxf5Rg0yK3AG?usp=sharing#scrollTo=4OHSwkDySV_-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1899d04-be08-4345-8648-83de7246d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da20b9e2-9f72-4191-b4d8-0ab188fdc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0c5575-921c-4fb9-996a-8af525723c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4421c271-4dd9-4b12-bd38-4d2a94042f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia llama-index-readers-wikipedia # Collect semantic data from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46938a9-2a0d-4281-9f17-43e0b31a9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU llama-index-vector-stores-qdrant qdrant-client #  vector database today will be powered by QDrant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca238c3-1dd7-4e7e-a1e8-2ce0eff2c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U sqlalchemy # Quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f877ffcb-151b-446b-92cd-7b9a495ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU llama-index-callbacks-wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77477698-e674-43b2-b500-b6bd8940f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb==0.16.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9491c6c-a621-45ba-ac00-4683b93d486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a3afae-7faf-4096-8756-09efb0ef26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import wandb\n",
    "PROJECT = \"mi-eval-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c1ca29-aeae-4276-9222-f659ff0a6049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37695fc7-14b2-48b5-8238-92e638c1ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgowri-apollo\u001b[0m (\u001b[33mapollo-messaging-intelligence\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shankar/work/turing/apollo/dev/ml/data-platform/notebooks/wandb/run-20240517_163821-fvzsriui</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm/runs/fvzsriui' target=\"_blank\">jumping-sponge-5</a></strong> to <a href='https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm' target=\"_blank\">https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm/runs/fvzsriui' target=\"_blank\">https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm/runs/fvzsriui</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(anonymous=\"allow\")\n",
    "run = wandb.init(project=PROJECT, job_type=\"agent\")\n",
    "\n",
    "# protobuf==3.20\n",
    "# wandb==0.16.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4316e2-c97f-4ca5-a5e2-0b5d79f8e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "547c54ed-c7d1-49b0-8e28-dea3c12aa3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"WandB API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc89798-07c7-4cd2-86b3-a5a71365bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.core import set_global_handler\n",
    "\n",
    "set_global_handler(\"wandb\", run_args={\"project\": PROJECT})\n",
    "wandb_callback = llama_index.core.global_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b25471-5f4e-4ea5-99e1-37c467e6a423",
   "metadata": {},
   "source": [
    "### Task 3: Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52918158-1348-43d5-9832-275c28899203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4053f0b-1976-4492-a0a1-ae6f80b9dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd654767-2687-40b1-83f3-1d01978740b9",
   "metadata": {},
   "source": [
    "### Index Creation using VectorStore\n",
    "Vector store index takes documents and splits them into nodes. Then it creates vector embeddings of the text of every node - now the nodes are ready to be queried.\n",
    "\n",
    "- Use wikipedia reader from llama_index and load the pages\n",
    "- Use locally hostable QDrant vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e99761-e61a-4802-a529-0aff4a4e5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.wikipedia import WikipediaReader \n",
    "\n",
    "movie_list = [\n",
    "    \"Dune (2021 film)\",\n",
    "    \"Dune: Part Two\",\n",
    "    \"The Lord of the Rings: The Fellowship of the Ring\",\n",
    "    \"The Lord of the Rings: The Two Towers\",\n",
    "]\n",
    "\n",
    "wiki_docs = WikipediaReader().load_data(pages=movie_list, auto_suggest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8265e88-1499-46d6-8470-9cd21d5ce15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name=\"movie_wikis\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65325137-b557-45e2-8325-eaf035f4a129",
   "metadata": {},
   "source": [
    "Create vector store and storage context -> allow us to create an empty vector store index that will be used to add nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68087955-8aca-4d88-97ec-f50a8e21097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"movie_wikis\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    [], storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66361bbb-f500-4fff-b517-25b6c4b03b9e",
   "metadata": {},
   "source": [
    "### Node Construction\n",
    "Loop through the documents and metadata and construct nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ffcac3-3878-4ea8-bea1-17128a8ffbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\n",
    "\n",
    "for movie, wiki_doc in zip(movie_list, wiki_docs):\n",
    "    nodes = pipeline.run(documents=[wiki_doc])\n",
    "    for node in nodes:\n",
    "        node.metadata = {\"title\": movie}\n",
    "    index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc99b6-5c86-4d36-b2f5-2c0b759ac3f9",
   "metadata": {},
   "source": [
    "### Simple RAG - QueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9abc7b13-1a82-4e20-8aa5-4c8907891755",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0096c198-2114-4873-8c1c-367ee609bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in simple_rag.get_prompts().items():\n",
    "    print(v.get_template())\n",
    "    print(\"\\n~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f46df9db-ea9b-4f73-93c7-0079acf76392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saruman the White is the evil wizard in the story.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = simple_rag.query(\"Who is the evil Wizard in the story?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e72c4-68f9-449b-8f97-e2a57a444cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "216f18ff-227f-421e-9573-90f3cf0fa780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The giant beings that roam across the world are the sandworms.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = simple_rag.query(\"Who are the giant beings that roam across the world?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "192dd70d-732a-4ec3-ab51-26e255e5c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dune (2021 film)', 'The Lord of the Rings: The Fellowship of the Ring']\n"
     ]
    }
   ],
   "source": [
    "print([x.metadata[\"title\"] for x in response.source_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3d0c1-1040-482a-9e01-ccad8175df51",
   "metadata": {},
   "source": [
    "### Auto Retriever Functional Tool\n",
    "To select correct metadata filter and query the filtered index. Nodes with desired metadata.\n",
    "\n",
    "- Create a vector store info object to hold the relevant metadata for each title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c9f207b-a9b5-4197-aa45-d3c45e58b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    VectorStoreInfo,\n",
    "    MetadataInfo,\n",
    "    ExactMatchFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from typing import List, Tuple, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"semantic information about movies\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            type=\"str\",\n",
    "            description='title of the movie, one of [\"Dune (2021 film)\", \"Dune: Part Two\", \"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\"]'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70f5cc48-af44-485d-8cd1-75a4b5e2a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRetrieveModel(BaseModel):\n",
    "    query: str = Field(..., description=\"natural language query string\")\n",
    "    filter_key_list: List[str] = Field(\n",
    "        ..., description=\"List of metadata filter field names\"\n",
    "    )\n",
    "    filter_value_list: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"List of metadata filter field values (corresponding to names specified in filter_key_list)\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d2a5d25-c678-453d-867c-b186819ef12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_retrieve_fn(\n",
    "    query: str, filter_key_list: List[str], filter_value_list: List[str]\n",
    "): \n",
    "    query = query or \"Query\"\n",
    "    exact_match_filters = [\n",
    "        ExactMatchFilter(key=k, value=v)\n",
    "        for k, v in zip(filter_key_list, filter_value_list)\n",
    "    ]\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index, filters=MetadataFilters(filters=exact_match_filters), top_k=top_k\n",
    "    )\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "    responses = query_engine.query(query)\n",
    "\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a98b9783-e837-4270-98cd-50d7916d6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = f\"\"\"\\\n",
    "Use this tool to look up non-review based information about films.\n",
    "The vector database schema is given below:\n",
    "{vector_store_info.json()}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3b40817-a61b-4e98-8615-a931cc025983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use this tool to look up non-review based information about films.\\nThe vector database schema is given below:\\n{\"metadata_info\": [{\"name\": \"title\", \"type\": \"str\", \"description\": \"title of the movie, one of [\\\\\"Dune (2021 film)\\\\\", \\\\\"Dune: Part Two\\\\\", \\\\\"The Lord of the Rings: The Fellowship of the Ring\\\\\", \\\\\"The Lord of the Rings: The Two Towers\\\\\"]\"}], \"content_info\": \"semantic information about movies\"}\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05ea7723-64aa-4242-b4e4-375d75545389",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_retrieve_tool = FunctionTool.from_defaults(\n",
    "    fn=auto_retrieve_fn,\n",
    "    name=\"semantic-film-info\",\n",
    "    description=description,\n",
    "    fn_schema=AutoRetrieveModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76b57cde-5df6-4793-9232-fda5bbef2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    tools=[auto_retrieve_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3d4b60b-09d6-4bbe-ba66-e5655daefe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Who starred in the 2021 film?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"cast of Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The giant beings that roam across the world are the sandworms.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"cast of Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The giant beings that roam across the world are the sandworms.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"cast of Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The giant beings that roam across the world are the sandworms.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"cast of Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The giant beings that roam across the world are the sandworms.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"cast of Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The giant beings that roam across the world are the sandworms.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"cast of Dune (2021 film)\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"Dune (2021 film)\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The giant beings that roam across the world are the sandworms.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the inconvenience. It seems there was an issue retrieving the cast information for the 2021 film \"Dune.\" Let me try to gather the cast details again.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Who starred in the 2021 film?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6912c4e0-d583-414a-98b3-6e0f89926062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Who are those giant guys from Lord of the Rings that roam around the forest?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.7121588826004072 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.7121588826004072 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 1.2124521560453674 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.2124521560453674 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.08754586332698056 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.08754586332698056 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 2.9032654437978973 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 2.9032654437978973 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 7.089794931746745 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 7.089794931746745 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "Failed to log trace tree to W&B: list index out of range\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are those giant guys from Lord of the Rings that roam around the forest?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:644\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    639\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    641\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    642\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 644\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat(\n\u001b[1;32m    645\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m    646\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[1;32m    647\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    648\u001b[0m         mode\u001b[38;5;241m=\u001b[39mChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT,\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[1;32m    651\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:574\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    571\u001b[0m dispatch_event(AgentChatWithStepStartEvent(user_msg\u001b[38;5;241m=\u001b[39mmessage))\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_step(\n\u001b[1;32m    575\u001b[0m         task\u001b[38;5;241m.\u001b[39mtask_id, mode\u001b[38;5;241m=\u001b[39mmode, tool_choice\u001b[38;5;241m=\u001b[39mtool_choice\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m    579\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:411\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 411\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mrun_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[1;32m    413\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/agent/openai/step.py:680\u001b[0m, in \u001b[0;36mOpenAIAgentWorker.run_step\u001b[0;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m tool_choice \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_step(\n\u001b[1;32m    681\u001b[0m     step, task, mode\u001b[38;5;241m=\u001b[39mChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT, tool_choice\u001b[38;5;241m=\u001b[39mtool_choice\n\u001b[1;32m    682\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/agent/openai/step.py:555\u001b[0m, in \u001b[0;36mOpenAIAgentWorker._run_step\u001b[0;34m(self, step, task, mode, tool_choice)\u001b[0m\n\u001b[1;32m    552\u001b[0m openai_tools \u001b[38;5;241m=\u001b[39m [tool\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mto_openai_tool() \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[1;32m    554\u001b[0m llm_chat_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_llm_chat_kwargs(task, openai_tools, tool_choice)\n\u001b[0;32m--> 555\u001b[0m agent_chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_agent_response(\n\u001b[1;32m    556\u001b[0m     task, mode\u001b[38;5;241m=\u001b[39mmode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_chat_kwargs\n\u001b[1;32m    557\u001b[0m )\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# TODO: implement _should_continue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m latest_tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_latest_tool_calls(task) \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/agent/openai/step.py:424\u001b[0m, in \u001b[0;36mOpenAIAgentWorker._get_agent_response\u001b[0;34m(self, task, mode, **llm_chat_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_agent_response\u001b[39m(\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m, task: Task, mode: ChatResponseMode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_chat_kwargs: Any\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AGENT_CHAT_RESPONSE_TYPE:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--> 424\u001b[0m         chat_response: ChatResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_chat_kwargs)\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_message(task, chat_response)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:148\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    133\u001b[0m     LLMChatStartEvent(\n\u001b[1;32m    134\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    141\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    142\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     },\n\u001b[1;32m    147\u001b[0m )\n\u001b[0;32m--> 148\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/llms/openai/base.py:296\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/llama_index/llms/openai/base.py:362\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m    361\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 362\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    363\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts,\n\u001b[1;32m    364\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    368\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    669\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    670\u001b[0m             {\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    691\u001b[0m             },\n\u001b[1;32m    692\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    693\u001b[0m         ),\n\u001b[1;32m    694\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    695\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    696\u001b[0m         ),\n\u001b[1;32m    697\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    698\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    699\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    700\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/openai/_base_client.py:1213\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1201\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1210\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1211\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/openai/_base_client.py:902\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    903\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    904\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    905\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    906\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    907\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    908\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mi-ml/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    992\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    996\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    997\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1001\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_eUe08KJB3t0nqCF4EzSUCJk9\", 'type': 'invalid_request_error', 'param': 'messages.[14].role', 'code': None}}"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Who are those giant guys from Lord of the Rings that roam around the forest?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2a30e-45ad-4e30-8812-85fcdaab010e",
   "metadata": {},
   "source": [
    "### Quantitative RAG Pipeline with NL2SQL Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8edc6-2e98-49f3-8762-6e196708a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/dune1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823bfeb3-bd0a-4352-b8fb-9465744376e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/dune2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cdffd-f59f-474f-abf1-edfe74b99aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/lotr_fotr.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b09731-e6bb-43e3-b48c-b5cc92ab8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/lotr_tt.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d660beed-9775-4b12-9733-fda9ab84bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dune1 = pd.read_csv(\"./dune1.csv\")\n",
    "dune2 = pd.read_csv(\"./dune2.csv\")\n",
    "lotr_fotr = pd.read_csv(\"./lotr_fotr.csv\")\n",
    "lotr_tt = pd.read_csv(\"./lotr_tt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da9b47f7-8238-4654-a13d-9ca05aa6833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite+pysqlite:///:memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c750c783-2ba2-4a66-a4ba-373f5f9281d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dune1.to_sql(\"Dune (2021 film)\", engine)\n",
    "dune2.to_sql(\n",
    "  \"Dune: Part Two\",\n",
    "  engine\n",
    ")\n",
    "lotr_fotr.to_sql(\n",
    "  \"The Lord of the Rings: The Fellowship of the Ring\",\n",
    "  engine\n",
    ")\n",
    "lotr_tt.to_sql(\n",
    "  \"The Lord of the Rings: The Two Towers\",\n",
    "  engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0758066-6e82-409e-8525-02ff955f5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SQLDatabase\n",
    "sql_database = SQLDatabase(\n",
    "    engine=engine,\n",
    "    include_tables=movie_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b426fc6-ab93-46eb-bdcd-190d8c9bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.struct_store.sql_query import NLSQLTableQueryEngine\n",
    "\n",
    "sql_query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=movie_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c3d9c85-8d31-4111-93ad-32f56766cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTION = \"\"\"This tool should be used to answer any and all review related inquiries by translating a natural language query into a SQL query with access to tables:\n",
    "'Dune (2021 film)' - containing info. about the first movie in the Dune series,\n",
    "'Dune: Part Two'- containing info. about about the second movie in the Dune series,\n",
    "'The Lord of the Rings: The Fellowship of the Ring' - containing info. about the first movie in the Lord of the Ring series,\n",
    "'The Lord of the Rings: The Two Towers' - containing info. the second movie in the Lord of the Ring series,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9074879-6d26-40ca-9111-5ddf02668528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools.query_engine import QueryEngineTool\n",
    "\n",
    "sql_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=sql_query_engine,\n",
    "    name=\"sql-query\",\n",
    "    description=DESCRIPTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b618239-b800-48db-b714-07b9fdbc88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(\n",
    "    tools=[sql_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a47f2fe-6013-42fa-b021-2e48d133f2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the average rating of the 2nd Lord of the Rings movie?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: sql-query with args: {\"input\":\"average rating of 'The Lord of the Rings: The Two Towers'\"}\n",
      "INFO:llama_index.core.indices.struct_store.sql_retriever:> Table desc str: Table 'Dune (2021 film)' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'Dune: Part Two' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Fellowship of the Ring' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Two Towers' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "> Table desc str: Table 'Dune (2021 film)' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'Dune: Part Two' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Fellowship of the Ring' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Two Towers' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The average rating of 'The Lord of the Rings: The Two Towers' is 9.18 out of 10.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average rating of the 2nd Lord of the Rings movie, \"The Lord of the Rings: The Two Towers,\" is 9.18 out of 10.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.chat(\"What is the average rating of the 2nd Lord of the Rings movie?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6acc7e05-8527-494c-91a8-73b032d4a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What movie series has better reviews, Lord of the Rings or Dune?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: sql-query with args: {\"input\": \"average rating of 'The Lord of the Rings: The Fellowship of the Ring'\"}\n",
      "INFO:llama_index.core.indices.struct_store.sql_retriever:> Table desc str: Table 'Dune (2021 film)' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'Dune: Part Two' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Fellowship of the Ring' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Two Towers' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "> Table desc str: Table 'Dune (2021 film)' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'Dune: Part Two' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Fellowship of the Ring' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Two Towers' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The average rating of 'The Lord of the Rings: The Fellowship of the Ring' is approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: sql-query with args: {\"input\": \"average rating of 'Dune (2021 film)'\"}\n",
      "INFO:llama_index.core.indices.struct_store.sql_retriever:> Table desc str: Table 'Dune (2021 film)' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'Dune: Part Two' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Fellowship of the Ring' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Two Towers' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "> Table desc str: Table 'Dune (2021 film)' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'Dune: Part Two' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Fellowship of the Ring' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "\n",
      "Table 'The Lord of the Rings: The Two Towers' has columns: index (BIGINT), Unnamed: 0 (BIGINT), Review_Date (TEXT), Author (TEXT), Rating (FLOAT), Review_Title (TEXT), Review (TEXT), Review_Url (TEXT), and foreign keys: .\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The average rating of 'Dune (2021 film)' is 8.34 out of 10.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average rating of \"The Lord of the Rings\" series is higher than that of the \"Dune\" series. \\n- \"The Lord of the Rings: The Fellowship of the Ring\" has an average rating of approximately 9.87 out of 10.\\n- \"Dune (2021 film)\" has an average rating of 8.34 out of 10.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.chat(\"What movie series has better reviews, Lord of the Rings or Dune?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16faf18c-a136-44b9-b724-04d3a4c24907",
   "metadata": {},
   "source": [
    "### Combined RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6ae06e7-872b-4610-87be-30710e59cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tool_agent = OpenAIAgent.from_tools(\n",
    "    tools=[auto_retrieve_tool, sql_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8551e97-b851-49d1-84f9-23fdb79f6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Which movie is about a ring, and what is the average rating of the movie?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\":\"movie about a ring\",\"filter_key_list\":[\"title\"],\"filter_value_list\":[\"The Lord of the Rings: The Fellowship of the Ring\",\"The Lord of the Rings: The Two Towers\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The average rating of \"The Lord of the Rings\" series is higher than that of the \"Dune\" series. \n",
      "- \"The Lord of the Rings: The Fellowship of the Ring\" has an average rating of approximately 9.87 out of 10.\n",
      "- \"Dune (2021 film)\" has an average rating of 8.34 out of 10.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = combined_tool_agent.chat(\"Which movie is about a ring, and what is the average rating of the movie?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "377964e9-2129-41f1-aa67-f051d88d7823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What worlds do the LoTR, and Dune movies take place in?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\": \"worlds in LoTR movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\": \"worlds in Dune movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"Dune (2021 film)\", \"Dune: Part Two\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\": \"worlds in LoTR movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\": \"worlds in Dune movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"Dune (2021 film)\", \"Dune: Part Two\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\": \"worlds in LoTR movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"The Lord of the Rings: The Fellowship of the Ring\", \"The Lord of the Rings: The Two Towers\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: semantic-film-info with args: {\"query\": \"worlds in Dune movies\", \"filter_key_list\": [\"title\"], \"filter_value_list\": [\"Dune (2021 film)\", \"Dune: Part Two\"]}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Got output: The movie about a ring is \"The Lord of the Rings: The Fellowship of the Ring.\" It has an average rating of approximately 9.87 out of 10.\n",
      "========================\n",
      "\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I apologize for the inconvenience. It seems there was an issue retrieving the information about the worlds in which the \"The Lord of the Rings\" and \"Dune\" movies take place. Let me try once more to provide you with the correct information.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = combined_tool_agent.chat(\"What worlds do the LoTR, and Dune movies take place in?\")\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fab3cce-a510-4873-ae3e-824e05887031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-sponge-5</strong> at: <a href='https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm/runs/fvzsriui' target=\"_blank\">https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm/runs/fvzsriui</a><br/> View project at: <a href='https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm' target=\"_blank\">https://wandb.ai/apollo-messaging-intelligence/mi-eval-llm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_163821-fvzsriui/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_callback.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc18ba-07aa-4f78-a146-eb70c614688d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a720b90-8a41-47c3-9d3e-515456ad662a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
